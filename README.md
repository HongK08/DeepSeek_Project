# LLM_FineTurning

# HAI ë‚´ì—ì„œ ì§„í–‰í•˜ëŠ” í•´ë‹¹ í”„ë¡œì íŠ¸ì— ëŒ€í•œ ë¬¸ì„œì„.
1. ì„œë²„ ë©”ë‰´ì–¼ì„ í•„ë… í›„ í† ì¹˜ ë˜ëŠ” í…ì„œì˜ ì›ë¦¬ë¥¼ ì´í•´ í•œ ì´í›„ì— ì ‘ê·¼ í•˜ëŠ” ê²ƒì„ ì¶”ì²œí•˜ëŠ” ë°”ì„
2. ê¸°ë³¸ì ìœ¼ë¡œ ì‚¬ìš©í•˜ëŠ” Unslothê³¼ ê·¸ì— ë”¸ë ¤ì˜¤ëŠ” Transformer ë“± í”„ë¡œê·¸ë¨ì´ ì‚¬ìš©í•˜ëŠ” CUDAì˜ ì›ë¦¬ë¥¼ ì´í•´í•˜ê³  ì‹œì‘í•˜ì—¬ì•¼ í•¨
3. ì‚¬ìš©í•˜ê²Œ ë˜ëŠ” Model ì€ DeepSeek-R1-Distill-Qwen-32B-unsloth-bnb-4bit ì´ë©° í•´ë‹¹ ëª¨ë¸ì€ R1ì˜ ì¦ë¥˜ ëª¨ë¸ì´ê³  32Bì´ë©° 4Bit ì–‘ìí™”ê°€ ëœ ëª¨ë¸ì´ë¼ëŠ” ëœ»ì„

4. 202032003 ê¸¸ê²½ë¯¼


# ì°¸ì¡°í•œ ë…¼ë¬¸
https://arxiv.org/pdf/2502.07316
https://arxiv.org/html/2412.19437v1#S3
https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-reasoning-llms 

# DeepSeek R1 ì •ë¦¬
DeepSeekì˜ ì¶”ë¡ ì—”ì§„  

í”íˆ ì•„ëŠ” LLMì˜ ì¶”ë¡  ë°©ì‹ì€ ì´ ë‘ê°€ì§€ ì…ë‹ˆë‹¤. <br/>
ìˆ˜í•™ë¬¸ì œ ë°ì´í„° : ë¬¸ì œë¥¼ ì£¼ê³  ì´ë¥¼ ìˆ˜í•™ì ìœ¼ë¡œ ì¶”ë¡ í•´ê°€ë©° ê³¼ì •ì„ í•™ìŠµí•˜ëŠ” ë°©ë²• <br/>
Chain-of-Thought ë°ì´í„° : ì‚¬ëŒ ìƒê°ì²˜ëŸ¼ í•´ê²° ë°©ì‹ íŠ¸ë¦¬ë¥¼ ì‚¬ìš©í•˜ëŠ” ë°©ì‹

DeepSeek ëŠ” ì´ê±¸ Codel/O ë¡œ í•™ìŠµì„ ì‹œí‚¨ê²Œ ì°¨ì´ì ì„ (ì¶”ë¡ ì—”ì§„)
ë§¥ë½ì—ì„œì˜ ë…¼ë¦¬ íë¦„ ê³„íš ìƒíƒœê³µê°„ íƒìƒ‰ ì˜ì‚¬ê²°ì • íŠ¸ë¦¬ ìˆœíšŒ ë“±ì„ íŠ¹ì§•ìœ¼ë¡œ ë´„
ì¦‰ ì´ê±¸ í†µí•´ì„œ ê¸°ë³¸ LLMë“¤ì˜ ì½”ë“œ ë¬¸ë²•ìœ¼ë¡œ ë¶€í„°ì˜ ë¶„ë¦¬ë¥¼ í•´ë‚´ê²Œ ë˜ì—ˆìŒ.

ê²°êµ­ì€ ìœ„ì—ì„œ ê¸°ìˆ í•œ ì „í†µì  í•™ìŠµë²•ì€ ê²°êµ­ ì¼ë°˜í™”ì˜ í•œê³„ê°€ ì¡´ì¬í•˜ê²Œ ë©ë‹ˆë‹¤.

Input/Output ì„ ì˜ˆì¸¡í•˜ì—¬ì„œ CoT ì¶”ë¡ ì„ ì‚¬ìš©í•˜ê²Œ ëœë‹¤ë©´,
ì´ëŠ” ê¸°ì¡´ ë¬¸ë²•ì—ì„œ ë²—ì–´ë‚˜ì„œ êµ¬ì¡°í™”ëœ ì¶”ë¡ ì„ ê°€ëŠ¥í•˜ê²Œ ë©ë‹ˆë‹¤.

ë˜í•œ ë™ì‹œì— ì ˆì°¨ì  ì—„ë°€ì„±ì„ ìœ ì§€í•˜ì—¬ ë‹¤ì–‘í•œ íŒ¨í„´ì˜ í•™ìŠµì´ ê°€ëŠ¥í•˜ê²Œ ë©ë‹ˆë‹¤.
í—ˆë‚˜ ì´ëŠ” ìˆ˜í•™ì  ì¶”ë¡ , ì½”ë“œ êµ¬ì„± ë“±ì—ì„œ ì´ì ì„ ê°€ì§€ë‚˜ ê·¸ ì™¸ì˜ ì¶”ë¡ ì—ì„œëŠ” ì„±ëŠ¥ ì €í•˜ë¥¼ ê°€ì ¸ì˜¤ëŠ” ë¬¸ì œê°€ ìˆìŠµë‹ˆë‹¤.
ê·¸ ì œì™¸ë˜ëŠ” ì¶”ë¡ ì˜ í•­ëª©ì€ ì¼ë°˜ì ì¸ ìì—°ì–´ CoT ë°ì´í„°ëŠ” ì¶”ë¡ ì´ ì´ë¯¸ ëª…í™•í•˜ê²Œ ì •ì˜ë˜ì–´ ìˆì§€ ì•Šì•„ ì¼ê´€ì„±ì´ ë¶€ì¡±í•  ìˆ˜ ìˆìœ¼ë©°,
ê·¸ë¦¬ê³  ìˆ˜í•™ ë¬¸ì œëŠ” íŠ¹ì •í•œ ìœ í˜•ì˜ ì¶”ë¡ ì—ë§Œ ì§‘ì¤‘í•˜ê¸° ë•Œë¬¸ì— ë” ë‹¤ì–‘í•œ ë…¼ë¦¬ì  ì‚¬ê³ ë¥¼ í•™ìŠµí•˜ê¸° ì–´ë µë‹¤. ë¼ëŠ” ë¬¸ì œë¥¼ ê°€ì§€ëŠ” ê²ƒ ì…ë‹ˆë‹¤.

ê¸°ì¡´ ë°©ë²•ê³¼ì˜ ì°¨ë³„ì 
ê¸°ì¡´ CoTëŠ” ì‚¬ëŒì´ ì‘ì„±í•œ ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ ì¼ê´€ì„±ì´ ì—†ì„ ìˆ˜ ìˆê³  í¸í–¥ëœ ì„±ëŠ¥ì„ ê°€ì§€ëŠ” ë¬¸ì œê°€ ìˆì—ˆìœ¼ë‚˜,
í•˜ì§€ë§Œ CodeI/Oì—ì„œëŠ” i/o ê´€ê³„ë¥¼ ì´ìš©í•˜ì—¬ CoT ìë™ ìƒì„±ì„ í•˜ê²Œ ë˜ì—ˆê³ ,
ì½”ë“œ ì‹¤í–‰ê³¼ì •ì€ ë…¼ë¦¬ì ìœ¼ë¡œ ì—„ë°€ ë‹¤ì–‘í•œ ì¶”ë¡  íŒ¨í„´ì„ í•™ìŠµì´ ê°€ëŠ¥í•´ì¡ŒìŠµë‹ˆë‹¤.

ë¬¸ì œ ì œê¸°
ê¸°ì¡´ì˜ LLMì€ ìˆ˜í•™ ë¬¸ì œ í•´ê²°ì´ë‚˜ ì½”ë“œ ìƒì„±ê³¼ ê°™ì€ íŠ¹ì • ë¶„ì•¼ì—ì„œëŠ” í’ë¶€í•œ í•™ìŠµ ë°ì´í„°ë¥¼ í†µí•´ ì„±ëŠ¥ì„ í–¥ìƒì‹œì¼œ ì˜´ í—ˆë‚˜ ë…¼ë¦¬ì  ì¶”ë¡ , ê³¼í•™ì  ì¶”ë¡ , ìƒì§•ì  ì¶”ë¡  ë“± ë‹¤ì–‘í•œ ì¶”ë¡  í•™ìŠµì—ì„œëŠ” í•™ìŠµ ë°ì´í„°ì˜ ë¶€ì¡±ê³¼ ë‹¨í¸ì„±ìœ¼ë¡œ ì„±ëŠ¥ í–¥ìƒì— í•œê³„ê°€ ìˆì—ˆìŠµë‹ˆë‹¤.

ì œì•ˆëœ ë°©ë²•
Code i/o ë¥¼ ì´ìš©í•˜ì—¬ ì½”ë“œì— ë‚´ì œëœ ë‹¤ì–‘í•œ ì¶”ë¡  íŒ¨í„´ì„ ì²´ê³„ì ìœ¼ë¡œ ì¶”ì¶œí•˜ì—¬ ìì—°ì–´ í˜•íƒœì˜ i/o ì˜ˆì¸¡ ë°ì´í„°ë¡œ ë³€í™˜í•˜ëŠ” ë°©ë²•ì„ ì œì•ˆí•˜ì˜€ëŠ”ë°

    def factorial(n):
            if n == 0:
                return 1
            return n * factorial(n-1)
            
    print(factorial(3))

ì´ê±¸ ìì—°ì–´ë¡œ ì´í•´ì‹œí‚¨ë‹¤ê³  ìƒê°í•´ì•¼ í•©ë‹ˆë‹¤ ì´ëŠ” í•˜ë‹¨ì˜ ê¸€ì²˜ëŸ¼ í’€ì´ê°€ ê°€ëŠ¥í•©ë‹ˆë‹¤.

factorial(3)ì„ í˜¸ì¶œí•œë‹¤. <br/>
3 != 0ì´ë¯€ë¡œ, 3 * factorial(2)ì„ ê³„ì‚°í•´ì•¼ í•œë‹¤. <br/>
factorial(2)ì„ í˜¸ì¶œí•œë‹¤. <br/>
2 != 0ì´ë¯€ë¡œ, 2 * factorial(1)ì„ ê³„ì‚°í•´ì•¼ í•œë‹¤. <br/>
factorial(1)ì„ í˜¸ì¶œí•œë‹¤. <br/>
1 != 0ì´ë¯€ë¡œ, 1 * factorial(0)ì„ ê³„ì‚°í•´ì•¼ í•œë‹¤. <br/>
factorial(0)ì„ í˜¸ì¶œí•œë‹¤. <br/>
0 == 0ì´ë¯€ë¡œ, factorial(0) = 1ì„ ë°˜í™˜í•œë‹¤. <br/>
ì´ë¥¼ ì´ìš©í•´, factorial(1) = 1 * 1 = 1ì´ ëœë‹¤. <br/>
ì´ë¥¼ ì´ìš©í•´, factorial(2) = 2 * 1 = 2ê°€ ëœë‹¤. <br/>
ì´ë¥¼ ì´ìš©í•´, factorial(3) = 3 * 2 = 6ì´ ëœë‹¤. <br/>

ì´ë¥¼ í†µí•˜ì—¬ ëª¨ë¸ì´ ì½”ë“œì˜ êµ¬ë¬¸ì— ì–½ë§¤ì´ì§€ ì•Šì€ ì±„ ë…¼ë¦¬ì  íë¦„ ê³„íš, ìƒíƒœ ê³µê°„ íƒìƒ‰,ì˜ì‚¬ ê²°ì • íŠ¸ë¦¬ ìˆœíšŒ, ëª¨ë“ˆì‹ ë¶„í•´ ë“±ì˜ ë²”ìš© ì¶”ë¡  ëŠ¥ë ¥ì„ í•™ìŠµì´ ê°€ëŠ¥í•´ì§€ëŠ” ì´ì•¼ê¸° ì…ë‹ˆë‹¤.

ê·¸ë¦¼ìœ¼ë¡œ ì„¤ëª…í•©ë‹ˆë‹¤.

            ROOT
             |
           [x < 5]
           /    \
       YES      NO
       /         \
    [x < 3]      [x < 8]
     /    \      /    \
    A      B    C      D

DFS/BFS ê°™ì€ ìˆœíšŒ
Root â†’ Left â†’ Left (A) â†’ Backtrack â†’ Right (B) â†’ Backtrack â†’ Right (C) â†’ Right (D)
Root â†’ Left â†’ Right â†’ Left Child (A, B) â†’ Right Child (C, D)

ì´ê±¸ CodeI/O ì—ì„œ ì ‘ê·¼í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ì„¤ëª…í•©ë‹ˆë‹¤.

    def decision_tree(x):
        if x < 5:
            if x < 3:
                return "A"
            else:
                return "B"
        else:
            if x < 8:
                return "C"
            else:
                return "D"
    
    print(decision_tree(2))  # "A"
    print(decision_tree(4))  # "B"
    print(decision_tree(6))  # "C"
    print(decision_tree(9))  # "D"

íŠ¸ë¦¬ êµ¬ì¡°ë¡œ ë³´ê²Œ ëœë‹¤ë©´.

             x < 5 ?
            /        \
        Yes          No
        /             \
     x < 3 ?         x < 8 ?
     /    \         /      \
    A      B       C        D

í•™ìŠµ ê³¼ì • <br/>
ëª¨ë¸ì€ ì£¼ì–´ì§„ code and TestCase ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ìì—°ì–´ í˜•íƒœì˜ CoT ì¶”ë¡ ì„ í†µí•˜ì—¬ 
i/o ë¥¼ ì˜ˆì¸¡í•˜ë„ë¡ í›ˆë ¨ì´ ë©ë‹ˆë‹¤. ì´ ê³¼ì •ì—ì„œ ëª¨ë¸ì€ êµ¬ì²´ì ì¸ êµ¬ë¬¸ì—ì„œ ë²—ì–´ë‚˜, ì ˆì°¨ì  ì—„ë°€ì„±ì„ ìœ ì§€í•˜ë©´ì„œë„ ë‹¤ì–‘í•œ ì¶”ë¡  íŒ¨í„´ì„ ë‚´ì¬í™” í•˜ê²Œ ë©ë‹ˆë‹¤.

ì˜ˆì‹œ ìƒí™© <br/>
ë¬¸ì œ : ì£¼ì–´ì§„ ì •ìˆ˜ Nì— ëŒ€í•´ 1ë¶€í„° ê¹Œì§€ì˜ ëª¨ë“  ìˆ˜ ì¤‘ì—ì„œ í™€ìˆ˜ë§Œ ì¶œë ¥í•˜ëŠ” í•¨ìˆ˜ë¥¼ ì‘ì„±í•˜ì‹­ì‹œì˜¤. ê·¸ë¦¬ê³  ë¦¬ìŠ¤íŠ¸ë¡œ ì¶œë ¥í•˜ì‹­ì‹œì˜¤

1ì•ˆ <br/>
ìš°ì„  inputì„ 5ë¡œ ì¤ë‹ˆë‹¤
ê·¸ëŸ¬ë©´ ë‹¹ì—°í•˜ê²Œë„ [1,3,5] ë¼ëŠ” ê°’ì„ ë°˜í™˜í•˜ê²Œ ë  ê²ë‹ˆë‹¤.

2ì•ˆ<br/>
input = 5 ë¡œ ë™ì¼í•©ë‹ˆë‹¤.

ë¬¸ì œ ì´í•´ = ìƒë‹¨ê³¼ ë™ì¼í•©ë‹ˆë‹¤
ì…ë ¥ ë¶„ì„ : ì…ë ¥ê°’ì€ 5ì…ë‹ˆë‹¤. -> ì´ëŠ” í•¨ìˆ˜ê°€ 1ë¶€í„° inputê°’ ê¹Œì§€ ìˆ˜ë¥¼ ê²€ì‚¬í•´ì•¼ í•œë‹¤ëŠ” ëœ» 
ì¶œë ¥ ì˜ˆì¸¡: 1,3,5 ëŠ” í™€ìˆ˜ë‹ˆê¹Œ (//2) í•´ì„œ ë‚˜ëˆ„ë©´ ë‹¹ì—°íˆ í™€ ì§ êµ¬ë¶„ì´ ë  í…Œë‹ˆê¹Œ ì´ë¥¼ Listì— append í•´ì£¼ëŠ” ê³¼ì •ì„ í•©ë‹ˆë‹¤

    def get_odds(N):
    return [x for x in range(1, N+1) if x % 2 == 1]

ì¦‰ ì´ í•¨ìˆ˜ëŠ” 1ë¶€í„° N(input) ê¹Œì§€ì˜ ìˆ˜ë¥¼ ë°˜ë³µí•˜ê³  í™€ìˆ˜ë§Œ ì–»ì–´ë‚´ê³  Listë¡œ Return í•´ ì£¼ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤.

ì´ëŸ° ë°©ì‹ìœ¼ë¡œ ë‘ê°€ì§€ë¡œ ì„œë¡œ ë‚˜ëˆ„ì–´ í•™ìŠµí•˜ëŠ” ê³¼ì •ì„ ê°€ì§€ëŠ” ì›ë¦¬ë¼ê³  ì •ë¦¬ê°€ ê°€ëŠ¥í•©ë‹ˆë‹¤.

ê²€ì¦ ë° ì„±ëŠ¥ í–¥ìƒ <br/>
ì—ì¸¡ëœ i/o ì„ ì‹¤ì œ ì •ë‹µê³¼ ë¹„êµí•˜ê±°ë‚˜ codeë¥¼ ì¬ì‹¤í–‰í•´ì„œ ê²°ê³¼ë¥¼ ê²€ì¦í•¨
í•´ë‹¹ ê³¼ì •ì„ í†µí•˜ì—¬ CoTë¥¼ ë‹¤ë‹¨ê³„ë¡œ ìˆ˜ì •í•˜ëŠ” CodeI/O++ ë°©ë²•ë„ ì¶”ê°€ì ìœ¼ë¡œ ì œì‹œí•©ë‹ˆë‹¤.

# ì •ë¦¬
ì œì•ˆ ëœ CodeI/o ë°©ì‹ì€ ìƒì§•ì , ê³¼í•™ì , ë…¼ë¦¬ì , ìˆ˜í•™ì  ì¶”ë¡  ì‘ì—…ì—ì„œ ì¼ê´€ëœ ì„±ëŠ¥ì˜ í–¥ìƒì„ í™•ì¸í•˜ì˜€ìŠµë‹ˆë‹¤.
ì´ëŠ” ë‹¤ì–‘í•œ ì¶”ë¡  íŒ¨í„´ì„ ìì—°ì–´ í˜•íƒœë¡œ í•™ìŠµí•˜ì—¬ ëª¨ë¸ì˜ ì „ë°˜ì ì¸ ì¶”ë¡  ëŠ¥ë ¥ì´ í–¥ìƒëœë‹¤ ë¼ëŠ” ë‚´ìš©ì…ë‹ˆë‹¤

# FINE_TURNING IN HAI
LLMì„ í•™ìŠµì‹œí‚¤ëŠ” ë°©ë²•ì€ ì§€ë„í•™ìŠµ/ë³´ê°•í•™ìŠµ/ë³´ìƒí•™ìŠµ ë“±ì˜ ë°©ë²•ë¡ ì´ ì—¬ëŸ¬ê°€ì§€ê°€ ìˆìŠµë‹ˆë‹¤.<br/>
í•˜ì§€ë§Œ ì´ë²ˆ íŒŒì¸íŠœë‹ì—ì„œ ì‚¬ìš©í•  ë¶€ë¶„ì€ ì§€ë„í•™ìŠµ ì…ë‹ˆë‹¤. ì¦‰ ê¸°ë³¸ DataSetì— ê°€ì ¸ì˜¨ Dataë¥¼ Push í•˜ì—¬ Promptë¥¼ ì£¼ëŠ” ìƒí™©ì—ì„œì˜ ë‹µì„ ë” ì •í™•í•˜ê²Œ ë§Œë“œëŠ” í–‰ìœ„ ì…ë‹ˆë‹¤.<br/>
ì´ë¥¼ ìœ„í•´ì„œ ìš°ë¦¬ëŠ” HuggingFace ë¼ëŠ” Ai í”Œë«í¼ì„ í™œìš© í•  ê²ƒì´ë©° ì´ê³³ì—ì„œ ë””ìŠ¤í‹¸ ëœ ëª¨ë¸ì„ ë‹¤ìš´ë°›ê³  ê±°ê¸°ì— ì›í•˜ëŠ” 1ì°¨ ë°ì´í„°ì…‹ì„ íŠœë‹ + ë”¥ì‹œí¬ì˜ ë¶€ì¡±í•œ ì–¸ì–´ ì„±ëŠ¥ì„ í•œêµ­ì–´ ì…‹ìœ¼ë¡œ íŠœë‹<br/>
ê·¸ ì´í›„ì— í•œêµ­ ì˜ë£Œì •ë³´ì™€ ê¸°íƒ€ í•„ìš”í•œ DataSetì„ Turning í›„ì— ì´ë¥¼ Quant í•˜ì—¬ ìµœì¢… ëª¨ë¸ ì¶•ì†Œê¹Œì§€ë¥¼ ëª©í‘œë¡œ í•˜ëŠ” í”„ë¡œì íŠ¸ì…ë‹ˆë‹¤<br/>

1. ì‚¬ìš©ì„ ìœ„í•œ í•œêµ­ì–´ ë°ì´í„°ì…‹ì˜ ë¶ˆëŸ¬ì˜¤ê¸° ê³¼ì • ì½”ë“œ
   
        import os
        import pandas as pd
        
        dir_path = './kor_eng' # ì—‘ì…€ íŒŒì¼ì´ ì €ì¥ëœ ë””ë ‰í„°ë¦¬
        files = os.listdir(dir_path)
        print(files)
        
        merge_df = pd.DataFrame()
        
        for file in files:
            df = pd.read_excel(f'{dir_path}/{file}')
            df = df[['ì›ë¬¸', 'ë²ˆì—­ë¬¸']]
            merge_df = pd.concat([merge_df, df])
        
        merge_df.columns = ['ko', 'en']
        merge_df.to_csv('./dataset.csv', index=False)

í•´ë‹¹ ì½”ë“œë¥¼ ì‚¬ìš©í•˜ê¸° ì „ì— ìš°ì„  AIí—ˆë¸Œì—ì„œ ì›í•˜ëŠ” ë²ˆì—­ íŒŒì¼ì„ ë‹¤ìš´ ë°›ì•„ì•¼ë§Œ í•©ë‹ˆë‹¤. <br/><br/>
https://www.aihub.or.kr/aihubdata/data/view.do?currMenu=115&topMenu=100&aihubDataSe=data&dataSetSn=126 <br/>

ì´í›„ì— í•´ë‹¹ ë°ì´í„°ì˜ í—¤ë”ê°’ì„ ì°¿ì•„ì„œ ì½”ë“œì— ë„£ê³  ì§€ì •í•´ì£¼ë©´ì„œ í•™ìŠµì„ ì‹œí‚¤ë©´ ë©ë‹ˆë‹¤ ì´ëŠ” ëª¨ë“  ë°ì´í„°ì…‹ í•™ìŠµ ë°©ë²•ê³¼ ìƒê¸° ë™ì¼í•©ë‹ˆë‹¤.  
í•˜ì§€ë§Œ HuggigFace(HF) ì—ì„œ import í•´ì„œ ì‚¬ìš©í•˜ëŠ” ë²•ë„ ìˆìŠµë‹ˆë‹¤ ì¶”í›„ì— ì „ì²´ ì†ŒìŠ¤ ì½”ë“œë¥¼ ë³´ë©° ì„¤ëª…ì„ ì´ì–´ ê°€ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤.  

ì´ì œ ì œê°€ ì‘ì„±í•œ ì½”ë“œë¥¼ ë³´ë©° ê¸°ë³¸ì ì¸ 4bit+ë””ìŠ¤í‹¸ ëª¨ë¸ì„ ì‚¬ìš©í•´ì„œ ì‚¬ìš©í•˜ëŠ” ë²•ì„ ì„¤ëª…í•˜ê² ìŠµë‹ˆë‹¤.  

1. ê¸°ë³¸ì ì¸ í™˜ê²½ ì„¤ì •  
    Unsloth ì´ ë¶€ë¶„ì€ ê·¸ëƒ¥ bashì—ì„œ pip install unsloth í•˜ì‹œë©´ ì¢…ì†ì„± ë¼ì´ë¸ŒëŸ¬ë¦¬ê¹Œì§€ ëª¨ë‘ ì„¤ì¹˜ë©ë‹ˆë‹¤.  
    ì´í›„ transformers, wandb ë“±ì„ ì„¤ì¹˜í•˜ë©° llama.cppëŠ” github ê³µì‹ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì°¸ê³ í•˜ì‹œì–´ ì„¤ì¹˜í•˜ì‹­ì‹œì˜¤.  
    Huggingfaceë„ ë§ˆì°¬ê°€ì§€ ì…ë‹ˆë‹¤. ë¬¼ë¡  ê°€ì¥ ê¸°ì´ˆì ì¸ TorchëŠ” ì„œë²„ ì„¤ì • ë©”ë‰´ì–¼ ì°¸ê³ í•˜ì…”ì•¼ í•©ë‹ˆë‹¤.  
   
2. ì´í›„ ì„¤ì •ì´ ë˜ì…¨ë‹¤ë©´  
    HF(HuggingFace) ë¡œê·¸ì¸ì„ í•˜ì…”ì•¼ í•©ë‹ˆë‹¤. ì´ë•Œ HFì— ê°€ì…í•˜ì‹œê³  ê°œì¸ í† í° í‚¤ ë°œê¸‰ë°›ìœ¼ì…”ì•¼ ì§„í–‰ ê°€ëŠ¥í•©ë‹ˆë‹¤.
    Token ìœ¼ë¡œ login í˜¸ì¶œí•˜ì…”ì„œ ë‹¤ìŒ seq ê¸¸ì´ì˜ ì§€ì • dtype ë“± ì§€ì •ê³¼ load_in_4bit ì€ ì‚¬ìš©í•˜ì‹œëŠ” ëª¨ë¸ì— ë”°ë¼ ì§€ì •í•˜ì‹­ì‹œì˜¤.
    íŠœë‹ ì „ prompt_style ì§€ì •í•´ì„œ íŠœë‹ ì „ ê²°ê³¼ê°’ ë³´ì…”ë„ ì¢‹ìŠµë‹ˆë‹¤. ì´ë•Œ ì§€ì •ëœ ì¸ìŠ¤íŠ¸ëŸ­ì…˜ ì§ˆë¬¸ ë‹µë³€ ë“± í˜•ì‹ì„ ìœ ì§€í•˜ì‹­ì‹œì˜¤.
    ê·¸ë¦¬ê³  ì§ˆë¬¸ì„ ì£¼ë©´ ì–˜ê°€ ë‹µë³€ì„ í•´ ì¤„ê²ë‹ˆë‹¤ ê¸°ë³¸ì ì¸ LLM ë‚´ì—ì„œ ì •ì œ ëœ R1ì´ ì£¼ëŠ” ë‹µë³€ì…ë‹ˆë‹¤.
    ì´í›„ Train í›„ì— ë‚˜ì˜¬ Promptì˜ ì–‘ì‹ë„ ìƒë‹¨ê³¼ ê°™ì´ ì§€ì •í•´ì•¼ í•©ë‹ˆë‹¤ ì´ëŠ” ì‚¬ìš© í•  ëª¨ë¸ì˜ êµ¬ì¡°ì™€ í•™ìŠµ ë°ì´í„°ì…‹ì„ ê³ ë ¤í•˜ì‹­ì‹œì˜¤.

3. ë°ì´í„° ì…‹ì˜ ì²˜ë¦¬ ì¤€ë¹„  
    ì´ì œ ìƒë‹¨ì—ì„œ ì§€ì •í•œ í”„ë¡¬í”„íŠ¸ ìŠ¤íƒ€ì¼ì„ formatting_prompts_func() ë¼ëŠ” í•­ëª©ì—ì„œ ì „ë¶€ ì§€ì •í•´ ì£¼ì–´ì•¼ë§Œ í•©ë‹ˆë‹¤.  
    ì´ê³³ì—ì„œ ê¸°ë³¸ì ì¸ êµ¬ì¡°ë¥¼ ì •ì˜í•˜ì‹œê³  ë‚œ ë‹¤ìŒì—ì•¼ ì €í¬ëŠ” ë°ì´í„°ì…‹ì„ ë¶ˆëŸ¬ì˜¤ê³  ê·¸ ë‚´ë¶€ì˜ í—¤ë”ê°’ì„ ì‚¬ìš©í•˜ì—¬ í•™ìŠµì„ ì‹œí‚¬ê²ë‹ˆë‹¤. (peft ì‚¬ìš©í•¨)  
    ê·¸ëŸ¬ë©´ ì´ì œ ì›í•˜ëŠ” ë°ì´í„°ì…‹ì˜ êµ¬ì¡°ë¥¼ ëª¨ë‘ íŒŒì•…í•˜ì˜€ë‹¤ëŠ” ê°€ì • í•˜ì— í•™ìŠµì„ ì§„í–‰ í•´ ë³´ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤.  
    from dataset import load_datasetì„ í•˜ì—¬ HFìƒì— ìˆëŠ” DataSetì„ ì‚¬ìš©í•œë‹¤ê³  ìƒê°í•˜ì‹œë©´ ë©ë‹ˆë‹¤  
        1. ê°€ì ¸ ì˜¬ ë°ì´í„°ì…‹ì˜ ì§ì ‘ ì£¼ì†Œ ë˜ëŠ” HF ë¼ì´ë¸ŒëŸ¬ë¦¬ ëª… + ë°ì´í„° ì…‹ ì´ë¦„  
        2. dataset.apì„ ì§€ì •í•´ì£¼ëŠ”ëŒ€ ì´ë•Œ ê·¸ ê°’ì€ ìœ„ì—ì„œ ì •ì˜í•œ formatting_prompt_func ì„ ì“°ê²Œ ë˜ëŠ”ê²ë‹ˆë‹¤.  
        3. datasetì—ì„œ êº¼ë‚´ ì“¸ ê²ƒ ì¦‰ ì²« ì½ëŠ” ìœ„ì¹˜ë¥¼ ì§€ì •í•˜ë©´ ëë‚©ë‹ˆë‹¤  
    ì´ì œ wandbë¥¼ ë¹„í™œì„±í™”(í•™ìŠµì— ì§€ì¥ì„ ì¤Œ) ì„ í™˜ê²½ë³€ìˆ˜ì— ~/.bashrcì— ì§€ì •í•˜ì‹œë˜ì§€ ì½”ë“œë‚´ì—ì„œ í•˜ë“œì§€ì • ê¼­ í•˜ì‹­ì‹œì˜¤ ì¤‘ìš”í•©ë‹ˆë‹¤.  
    ê·¸ë¦¬ê³  í´ë˜ìŠ¤ ì•„ë˜ì— ìƒì„±ì ë§Œë“œì…”ì„œ ì „ë¶€ í•˜ë‚˜í•˜ë‚˜ ì§€ì •í•´ ì£¼ì–´ì•¼ í•©ë‹ˆë‹¤ ì´ëŠ” ì½”ë“œ ì°¸ê³ í•˜ì‹œë©´ ì¢‹ìŠµë‹ˆë‹¤.  
    ê·¸ë¦¬ê³  í† í¬ë‚˜ì´ì§• í›„ì— returnìœ¼ë¡œ ë°˜í™˜ê¹Œì§€ í•´ì£¼ë©´ ì •ë¦¬ê°€ ë©ë‹ˆë‹¤.  

4. Train    
    ì„¤ì •ì´ ëª¨ë‘ ì™„ë£Œë˜ì—ˆìœ¼ë‹ˆ ì´ì œ í•™ìŠµì„ ì‹œí‚¬ ì°¨ë¡€ì…ë‹ˆë‹¤.
    ì—¬ê¸°ì„œ bach_size/gradient ë“± ê°’ì„ ëª¨ë‘ ì›í•˜ëŠ”ëŒ€ë¡œ ì§€ì •í•˜ì…”ì•¼ í•˜ëŠ”ë° ì´ë•Œ ì´ ê°’ì€ í˜„ì¬ Serverì˜ GPU V-RAM ìš©ëŸ‰ì„ ê³ ë ¤í•˜ì‹œë©° ì§€ì •í•˜ì‹­ì‹œì˜¤.  
    ê°„ë‹¨í•˜ê²Œ ì´ ê°’ì´ ë†’ì•„ì§ˆìˆ˜ë¡ GPUì˜ ì‚¬ìš©ìœ¨ì´ ë†’ì•„ì§ê³¼ ë™ì‹œì— Trainì—ì„œ Epochì—ë„ ì˜í–¥ì„ ì£¼ê²Œ ë©ë‹ˆë‹¤.  
    ê·¸ë¦¬ê³  learnig_rate= ê°’ë„ ë³¸ì¸ì´ ê³ ë ¤í–ì—¬ ì§€ì •, ì—°ì‚° ë°©ì‹ì€ bf16/fp16ì´ ìˆëŠ”ë° êµ¬í˜• GPUëŠ” fp16ë§Œ ì§€ì›í•˜ë‚˜ Serverì˜ 4090ì€ bf16ì„ ì§€ì›í•©ë‹ˆë‹¤.  
    ê·¸ë¦¬ê³  ê·¸ ì™¸ì˜ ê°’ì€ êµ¬ê¸€ë§ì„ í†µí•˜ì—¬ ì™œ ì´ë ‡ê²Œ ë˜ê³  ì–´ë–¤ ë°©ì‹ì´ ë‚´ ëª¨ë¸ì— ì í•©í•œê°€ë¥¼ ìƒê°í•˜ì‹œê³  Seedê°’ì€ ëœë¤ìœ¼ë¡œ ë‘ì‹œë˜ì§€ ììœ ì…ë‹ˆë‹¤.  

    ì´í›„ trainer í˜¸ì¶œí•´ì„œ ëŒë ¤ì£¼ë©´ ë©ë‹ˆë‹¤.  
    ê·¸ëŸ¼ ë‹¹ì—°íˆ ë‚˜ì˜¨ ëª¨ë¸ì´ ë°”ë€ ê°’ì„ ì˜ ì¶”ë¡ í•´ì£¼ëŠ”ê°€? ì´ê±¸ í™•ì¸í•˜ê³  ì‹¶ìœ¼ì‹¤ê²ë‹ˆë‹¤.  
    ë§ˆì°¬ê°€ì§€ë¡œ ìƒë‹¨ì˜ ì§€ì •í•˜ì‹  prompt ë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ì‹œë©° ì¸ìŠ¤íŠ¸ëŸ­ì…˜ì€ Dataset ì œê³µìì˜ ê¶Œì¥ì‚¬í•­ì„ ë”°ë¥´ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤.  
    ê·¸ë¦¬ê³  ì´ë•Œ ì•„ì›ƒí’‹ì´ë‚˜ ì¸ë±ìŠ¤ê°’ì€ ë‹¹ì—°íˆ ë°›ê³ ì‹¶ì€ prompt ì— ë§ê²Œ ì§€ì •í•˜ì…”ì•¼ í•˜ë©° to("cuda") ì§€ì •í•˜ì‹œì–´ ì‚¬ìš©í•˜ì‹­ì‹œì˜¤  

    
    ê·¸ ì´í›„ ë¡œì»¬ì— quant í•˜ì‹œì–´ ì €ì¥í•˜ì‹œê±°ë‚˜ HFì— create_repo/repo_name ì§€ì •í•˜ì‹œì–´ push í•˜ì‹œë©´ ê¸°ë³¸ì ì¸ íŒŒì¸íŠœë‹ì´ ëë‚˜ê²Œ ë©ë‹ˆë‹¤.
    
        new_model_local = "<name>"
        model.save_pretrained(new_model_local)
        tokenizer.save_pretrained(new_model_local)
        
        model.save_pretrained_merged(new_model_local, tokenizer, save_method = "merged_16bit",) 
        model.push.to.hub_gguf(repo.name,tokenizer, quantization_method="<llama.cpp ë¬¸ì„œ ì°¸ì¡°í•˜ì—¬ ê¸°ì…>")


# ì£¼ì„ì´ ëª¨ë‘ ë‹¬ë¦° ì²« ë² ì´ìŠ¤ ì½”ë“œ
    
    from huggingface_hub import login
    from getpass import getpass
    
    #í† í° ì…ë ¥ (ë¹„ë°€ë²ˆí˜¸ì²˜ëŸ¼ ì…ë ¥ë¨)
    hf_token = '<YOUR_TOKEN>'
    login(token=hf_token) #ê·€ì°®ì•„ì„œ ê·¸ëƒ¥ ê°•ì œë¡œ ì§€ì •í•´ë²„ë ¸ìŠµë‹ˆë‹¤.

    #ëª¨ë¸ ë¡œë“œ
    from unsloth import FastLanguageModel
    
    #ëª¨ë¸ ì„¤ì •ê°’ ì •ì˜
    max_seq_length = 2048
    dtype = None
    load_in_4bit = True
    
    # ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë“œ
    model, tokenizer = FastLanguageModel.from_pretrained(
        model_name="unsloth/DeepSeek-R1-Distill-Qwen-7B-unsloth-bnb-4bit",  # ì •í™•í•œ ëª¨ë¸ëª…
        max_seq_length=max_seq_length,
        dtype=dtype,
        load_in_4bit=load_in_4bit,
        token=None
    )
    
    print("ì‚´ë ¤ì£¼ì„¸ìš”!")
    prompt_style = """Below is an instruction that describes a task, paired with an input that provides further context.
    Write a response that appropriately completes the request.
    Before answering, think carefully about the question and create a step-by-step chain of thoughts to ensure a logical and accurate response.
    
    ### Instruction:
    You are a medical expert with advanced knowledge in clinical reasoning, diagnostics, and treatment planning.
    Please answer the following medical question.
    
    ### Question:
    {}
    
    ### Response:
    <think>{}"""

    question = "A 55-year-old extremely obese man experiences weakness, sweating, tachycardia, confusion, and headache when fasting for a few hours, which are relieved by 
     eating. What disorder is most likely causing these symptoms?"


    FastLanguageModel.for_inference(model)
    inputs = tokenizer([prompt_style.format(question, "")], return_tensors="pt").to("cuda")
    
    outputs = model.generate(
        input_ids=inputs.input_ids,
        attention_mask=inputs.attention_mask,
        max_new_tokens=1200,
        use_cache=True,
    )
    response = tokenizer.batch_decode(outputs)
    print(response[0].split("### Response:")[1])

    #DataSet Loading
    
    train_prompt_style = """Below is an instruction that describes a task, paired with an input that provides further context.
    Write a response that appropriately completes the request.
    Before answering, think carefully about the question and create a step-by-step chain of thoughts to ensure a logical and accurate response.
    
    ### Instruction:
    You are a medical expert with advanced knowledge in clinical reasoning, diagnostics, and treatment planning.
    Please answer the following medical question.
    
    ### Question:
    {}
    
    ### Response:
    <think>
    {}
    </think>
    {}"""

    EOS_TOKEN = tokenizer.eos_token  # Must add EOS_TOKEN


    def formatting_prompts_func(examples):
        inputs = examples["Question"]
        cots = examples["Complex_CoT"]
        outputs = examples["Response"]
        texts = []
        for input, cot, output in zip(inputs, cots, outputs):
            text = train_prompt_style.format(input, cot, output) + EOS_TOKEN
            texts.append(text)
        return {
            "text": texts,
        }

    #Model setup

    from datasets import load_dataset
    dataset = load_dataset("FreedomIntelligence/medical-o1-reasoning-SFT","en", split = "train[0:700]",trust_remote_code=True)
    dataset = dataset.map(formatting_prompts_func, batched = True,)
    dataset["text"][0]

    model = FastLanguageModel.get_peft_model(
    model,
    r=16,
    target_modules=[
        "q_proj",
        "k_proj",
        "v_proj",
        "o_proj",
        "gate_proj",
        "up_proj",
        "down_proj",
    ],
    lora_alpha=16,
    lora_dropout=0,
    bias="none",
    use_gradient_checkpointing="unsloth",  # True or "unsloth" for very long context
    random_state=3407,
    use_rslora=False,
    loftq_config=None,
    )

    # Train

    import os
    import wandb
    from transformers import Trainer, TrainingArguments
    from torch.utils.data import Dataset
    
    #wandb ë¹„í™œì„±í™”
    os.environ["WANDB_DISABLED"] = "true"
    wandb.init(mode="disabled")

    class CustomDataset(Dataset):
        def __init__(self, dataset, tokenizer, max_length):
            self.dataset = dataset
            self.tokenizer = tokenizer
            self.max_length = max_length
    
        def __len__(self):
            return len(self.dataset)
    
        def __getitem__(self, idx):
            item = self.dataset[idx]
            text = item['text']
    
            # í† í¬ë‚˜ì´ì§•
            encodings = self.tokenizer(
                text,
                truncation=True,
                max_length=self.max_length,
                padding='max_length',
                return_tensors='pt'
            )
    
            # labels ì¶”ê°€ (input_idsì™€ ë™ì¼í•˜ê²Œ ì„¤ì •)
            return {
                'input_ids': encodings['input_ids'].squeeze(),
                'attention_mask': encodings['attention_mask'].squeeze(),
                'labels': encodings['input_ids'].squeeze()  # labels ì¶”ê°€
            }
    
    processed_dataset = CustomDataset(dataset, tokenizer, max_seq_length)
    
        training_args = TrainingArguments(
            output_dir="outputs",
            per_device_train_batch_size=2,
            gradient_accumulation_steps=4,
            warmup_steps=5,
            max_steps=60,
            learning_rate=2e-4,
            fp16=False,
            bf16=True,
            logging_steps=10,
            optim="adamw_torch",
            weight_decay=0.01,
            lr_scheduler_type="linear",
            seed=3407,
            report_to="none",
            remove_unused_columns=False
        )
        
        trainer = Trainer(
            model=model,
            args=training_args,
            train_dataset=processed_dataset,
        )
        
    trainer.train()
    
        # After Fine_Turning
    
        question = "A 55-year-old extremely obese man experiences weakness, sweating, tachycardia, confusion, and headache when fasting for a few hours, which are relieved by eating. What disorder is most likely causing these symptoms?"
    
    
        FastLanguageModel.for_inference(model)  # Unsloth has 2x faster inference!
        inputs = tokenizer([prompt_style.format(question, "")], return_tensors="pt").to("cuda")
        
        outputs = model.generate(
            input_ids=inputs.input_ids,
            attention_mask=inputs.attention_mask,
            max_new_tokens=1200,
            use_cache=True,
        )
        response = tokenizer.batch_decode(outputs)
        print(response[0].split("### Response:")[1])
    
        new_model_local = "<name>"
        model.save_pretrained(new_model_local)
        tokenizer.save_pretrained(new_model_local)
        
        model.save_pretrained_merged(new_model_local, tokenizer, save_method = "merged_16bit",)
    
        from huggingface_hub import login, create_repo
        import os
        
        # 1. í™˜ê²½ë³€ìˆ˜ ì„¤ì •
        os.environ["HUGGINGFACE_TOKEN"] = "hf_your_token"
        login(token=hf_token)  # í† í°ìœ¼ë¡œ ë¡œê·¸ì¸
        
        # 2. ìƒˆ ì €ì¥ì†Œ ìƒì„± í›„ ì—…ë¡œë“œ
        repo_name = "<YOYR_REPO/NAME>"
        create_repo(repo_name)
        model.push_to_hub_gguf(repo_name, tokenizer, quantization_method="q4_k_m")

# ì´ë ‡ê²Œ í•˜ì—¬ 1ì°¨ íŠœë‹ì„ ì™„ë£Œí•©ë‹ˆë‹¤.
    
Q4ë¡œ ì €ì¥í–ˆìœ¼ë‚˜ ì €í¬ëŠ” ì´í›„ ê°’ ë¹„êµë¥¼ ìœ„í•´ í•œêµ­ì–´ ë°ì´í„° ì…‹ì„ ì¶”ê°€ì ìœ¼ë¡œ í•™ìŠµì‹œí‚¬ ì˜ˆì •ì…ë‹ˆë‹¤


    from numba import cuda
    from datasets import load_dataset
    from transformers import Trainer, TrainingArguments
    from torch.utils.data import Dataset
    from huggingface_hub import login
    from unsloth import FastLanguageModel
    import os
    import wandb
    
    # CUDA ë¦¬ì…‹ (ë©”ëª¨ë¦¬ ì´ˆê¸°í™”)
    device = cuda.get_current_device()
    device.reset()
    
    # Hugging Face ë¡œê·¸ì¸
    hf_token = "your_huggingface_token"
    login(token=hf_token)
    
    # ëª¨ë¸ ì„¤ì •
    max_seq_length = 2048
    dtype = None
    load_in_4bit = True
    
    # ëª¨ë¸ ë¡œë“œ
    model, tokenizer = FastLanguageModel.from_pretrained(
        model_name="unsloth/DeepSeek-R1-Distill-Qwen-14B-unsloth-bnb-4bit",
        max_seq_length=max_seq_length,
        dtype=dtype,
        load_in_4bit=load_in_4bit,
        token=None
    )
    print("ëª¨ë¸ ë¡œë”© ì™„ë£Œ!")
    
    # ë°ì´í„°ì…‹ ë¡œë”©
    dataset = load_dataset("MarkrAI/KoCommercial-Dataset", split="train")
    print("ë°ì´í„°ì…‹ ë¡œë”© ì™„ë£Œ!")
    
    # ë°ì´í„°ì…‹ ë³€í™˜ í•¨ìˆ˜
    def format_data(example):
        instruction = example.get("instruction", "ë‹¤ìŒ ê´‘ê³  ë¬¸êµ¬ë¥¼ ë¶„ì„í•˜ì„¸ìš”.")
        input_text = example.get("input", "")
        output_text = example.get("output", example.get("text", ""))  # 'text' í•„ë“œë„ ê³ ë ¤
    
        return {
            "text": f"### Instruction:\n{instruction}\n\n### Input:\n{input_text}\n\n### Response:\n{output_text}"
        }
    
    # ë°ì´í„° ë³€í™˜ ì ìš©
    formatted_dataset = dataset.map(format_data)
    
    # ë°ì´í„°ì…‹ í´ë˜ìŠ¤ ì •ì˜
    class CustomDataset(Dataset):
        def __init__(self, dataset, tokenizer, max_length):
            self.dataset = dataset
            self.tokenizer = tokenizer
            self.max_length = max_length
    
        def __len__(self):
            return len(self.dataset)
    
        def __getitem__(self, idx):
            item = self.dataset[idx]
            text = item['text']
    
            encodings = self.tokenizer(
                text,
                truncation=True,
                max_length=self.max_length,
                padding='max_length',
                return_tensors='pt'
            )
    
            return {
                'input_ids': encodings['input_ids'].squeeze(),
                'attention_mask': encodings['attention_mask'].squeeze(),
                'labels': encodings['input_ids'].squeeze()
            }
    
    # ë³€í™˜ëœ ë°ì´í„°ì…‹ì„ CustomDatasetìœ¼ë¡œ ì²˜ë¦¬
    processed_dataset = CustomDataset(formatted_dataset, tokenizer, max_seq_length)
    
    # í›ˆë ¨ ì„¤ì •
    training_args = TrainingArguments(
        output_dir="outputs",
        per_device_train_batch_size=1,
        gradient_accumulation_steps=4,
        warmup_steps=10,
        max_steps=1000,
        learning_rate=2e-5,
        fp16=False,
        logging_steps=10,
        optim="adamw_torch",
        weight_decay=0.01,
        lr_scheduler_type="linear",
        seed=3407,
        report_to="none",
        remove_unused_columns=False,
    )
    
    # wandb ë¹„í™œì„±í™”
    os.environ["WANDB_DISABLED"] = "true"
    wandb.init(mode="disabled")
    
    # íŠ¸ë ˆì´ë„ˆ ì„¤ì •
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=processed_dataset,
    )
    
    # í›ˆë ¨ ì‹œì‘
    trainer.train()
    
    # í›ˆë ¨ ì™„ë£Œ í›„ ëª¨ë¸ ì €ì¥
    model.push_to_hub("HongKi08/Korean-Qwen14B", tokenizer, quantization_method="q4_k_m")
    print("ëª¨ë¸ ì—…ë¡œë“œ ì™„ë£Œ!")

ì´ ë–„ ì–‘ìí™” ì¡°ê±´ì´ë¼ë˜ê°€ ê·¸ëŸ° êµ¬ë¬¸ì€ ìƒí™©ì— ë§ê²Œ ë³€ê²½í•˜ì—¬ ë¡œì»¬ì— ì €ì¥í•˜ë˜ ì—…ë¡œë“œí•˜ë˜ ê°œì¸ì˜ ì„ íƒì…ë‹ˆë‹¤.

    repo_name = "HongKi08/SanHak"
    model.push_to_hub(repo_name)
    tokenizer.push_to_hub(repo_name)






#
    import os
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel
from huggingface_hub import HfApi, login
import subprocess
import shutil

# Hugging Face ë¡œê·¸ì¸
hf_token = "hf_..."  # ë„ˆì˜ í† í°ìœ¼ë¡œ ë°”ê¿”ì¤˜
login(hf_token)

# ì„¤ì •
base_model_id = "deepseek-ai/DeepSeek-R1-Distill-Qwen-14B"
lora_model_id = "HongKi08/14B_KOR_MED_VAL"
merged_dir = "./merged_14b_kor_med"
gguf_dir = "./gguf_model"
repo_id_transformers = "HongKi08/14B_KOR_MED_MERGED"
repo_id_gguf = "HongKi08/14B_KOR_MED_GGUF"

# 1. ë³‘í•©
print("â–¶ï¸ ë³‘í•© ì¤‘...")
base_model = AutoModelForCausalLM.from_pretrained(
    base_model_id, device_map="auto", torch_dtype="auto"
)
model = PeftModel.from_pretrained(base_model, lora_model_id)
merged_model = model.merge_and_unload()

# 2. ì €ì¥
print("ğŸ’¾ ì €ì¥ ì¤‘...")
tokenizer = AutoTokenizer.from_pretrained(lora_model_id)
merged_model.save_pretrained(merged_dir)
tokenizer.save_pretrained(merged_dir)

# 3. Hugging Faceì— ì—…ë¡œë“œ (transformers format)
print("â¬†ï¸ Hugging Face ì—…ë¡œë“œ ì¤‘ (transformers)...")
api = HfApi()
api.create_repo(repo_id_transformers, exist_ok=True)
merged_model.push_to_hub(repo_id_transformers)
tokenizer.push_to_hub(repo_id_transformers)

# 4. GGUF ë³€í™˜ (llama.cpp ì‚¬ìš©)
print("ğŸ” GGUF ë³€í™˜ ì¤‘...")
gguf_script = "./llama.cpp/convert.py"
if not os.path.exists("llama.cpp"):
    subprocess.run(["git", "clone", "https://github.com/ggerganov/llama.cpp.git"])

subprocess.run([
    "python3", gguf_script,
    merged_dir,
    "--outfile", f"{gguf_dir}/model.gguf"
])

# tokenizerë„ ë³µì‚¬
shutil.copytree(os.path.join(merged_dir, "tokenizer_config.json"), os.path.join(gguf_dir, "tokenizer_config.json"), dirs_exist_ok=True)

# 5. Hugging Faceì— ì—…ë¡œë“œ (gguf)
print("â¬†ï¸ Hugging Face ì—…ë¡œë“œ ì¤‘ (GGUF)...")
api.create_repo(repo_id_gguf, exist_ok=True)
api.upload_folder(
    folder_path=gguf_dir,
    repo_id=repo_id_gguf,
    repo_type="model"
)

print("âœ… ëª¨ë“  ê³¼ì • ì™„ë£Œ!")


í•™ìŠµ -> ë‹¤ì‹œ ë¶ˆëŸ¬ì˜´ -> í•™ìŠµ ì˜ ê³¼ì •ì„ ì§„í–‰í•˜ê³  ì‹¶ë‹¤ë©´ ì´ëŸ°ì‹ìœ¼ë¡œ Transformer ê°€ ì‚¬ìš© ê°€ëŠ¥í•˜ë„ë¡ ì§€ì •í•˜ì—¬ push í•´ì¤ë‹ˆë‹¤.

ì´ ê³¼ì •ì„ ë°˜ë³µí•˜ë©´ì„œ ê° í•™ìŠµ ì§„í–‰ë„ì— ë”°ë¥¸ ë²¤ì¹˜ë§ˆí¬ì˜ ê²°ê³¼ë¥¼ ë³´ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤.

